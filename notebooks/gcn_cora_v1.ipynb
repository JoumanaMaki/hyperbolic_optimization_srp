{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ace95b",
   "metadata": {},
   "source": [
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. \n",
    "\n",
    "The citation network consists of 5429 links. \n",
    "\n",
    "Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. \n",
    "\n",
    "The dictionary consists of 1433 unique words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "70d8433c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bcecfc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import torch\n",
    "from torch_geometric.datasets import TUDataset, Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "from models.node_clf.gcn.basic_gcn import GCN\n",
    "from utils.seed_everything import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "124c86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f5133df7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: Cora():\n",
      "====================\n",
      "Number of graphs: 1\n",
      "Number of features: 1433\n",
      "Number of classes: 7\n",
      "\n",
      "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n",
      "=============================================================\n",
      "Number of nodes: 2708\n",
      "Number of edges: 10556\n",
      "Average node degree: 3.90\n",
      "Has isolated nodes: False\n",
      "Has self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "source": [
    "dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures(), split=\"public\")\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'Has self-loops: {data.has_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a141709",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 2, 3, 4, 5, 6]), array([351, 217, 418, 818, 426, 298, 180]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(data.y, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db973945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10556])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00e19ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 633, 1862, 2582,  ...,  598, 1473, 2706],\n",
       "        [   0,    0,    0,  ..., 2707, 2707, 2707]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adjacency matrix in COO format\n",
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2deadcf9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2708]), 2708)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node classes (target)\n",
    "data.y.shape, data.num_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6887af8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708, 1433])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# node features, each node has a bag-of-wrods of size 1433\n",
    "data.x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "506a7ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2708])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2c167145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10556])\n",
      "Number of unique undirected edges: 5278\n"
     ]
    }
   ],
   "source": [
    "edge_index = data.edge_index\n",
    "print(edge_index.shape)  # [2, 10556]\n",
    "\n",
    "# Count unique undirected edges\n",
    "edges = edge_index.t().tolist()  # list of (source, target)\n",
    "undirected_edges = set(tuple(sorted(edge)) for edge in edges)\n",
    "\n",
    "print(\"Number of unique undirected edges:\", len(undirected_edges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d80abf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(12345)\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d76ddb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1:\n",
      "=======\n",
      "Number of graphs in the current batch: 1\n",
      "DataBatch(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], batch=[2708], ptr=[2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9891c432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 1.9470\n",
      "Epoch: 002, Loss: 1.9373\n",
      "Epoch: 003, Loss: 1.9294\n",
      "Epoch: 004, Loss: 1.9152\n",
      "Epoch: 005, Loss: 1.9024\n",
      "Epoch: 006, Loss: 1.8836\n",
      "Epoch: 007, Loss: 1.8642\n",
      "Epoch: 008, Loss: 1.8526\n",
      "Epoch: 009, Loss: 1.8231\n",
      "Epoch: 010, Loss: 1.7978\n",
      "Epoch: 011, Loss: 1.7799\n",
      "Epoch: 012, Loss: 1.7559\n",
      "Epoch: 013, Loss: 1.7235\n",
      "Epoch: 014, Loss: 1.6921\n",
      "Epoch: 015, Loss: 1.6722\n",
      "Epoch: 016, Loss: 1.6488\n",
      "Epoch: 017, Loss: 1.6327\n",
      "Epoch: 018, Loss: 1.5713\n",
      "Epoch: 019, Loss: 1.5630\n",
      "Epoch: 020, Loss: 1.5041\n",
      "Epoch: 021, Loss: 1.4726\n",
      "Epoch: 022, Loss: 1.4570\n",
      "Epoch: 023, Loss: 1.3976\n",
      "Epoch: 024, Loss: 1.3842\n",
      "Epoch: 025, Loss: 1.3258\n",
      "Epoch: 026, Loss: 1.2797\n",
      "Epoch: 027, Loss: 1.2565\n",
      "Epoch: 028, Loss: 1.2375\n",
      "Epoch: 029, Loss: 1.1921\n",
      "Epoch: 030, Loss: 1.1341\n",
      "Epoch: 031, Loss: 1.0790\n",
      "Epoch: 032, Loss: 1.0791\n",
      "Epoch: 033, Loss: 1.0165\n",
      "Epoch: 034, Loss: 1.0104\n",
      "Epoch: 035, Loss: 0.9770\n",
      "Epoch: 036, Loss: 0.9094\n",
      "Epoch: 037, Loss: 0.9686\n",
      "Epoch: 038, Loss: 0.8357\n",
      "Epoch: 039, Loss: 0.8466\n",
      "Epoch: 040, Loss: 0.7974\n",
      "Epoch: 041, Loss: 0.7651\n",
      "Epoch: 042, Loss: 0.7559\n",
      "Epoch: 043, Loss: 0.7483\n",
      "Epoch: 044, Loss: 0.7325\n",
      "Epoch: 045, Loss: 0.6864\n",
      "Epoch: 046, Loss: 0.6683\n",
      "Epoch: 047, Loss: 0.6520\n",
      "Epoch: 048, Loss: 0.6410\n",
      "Epoch: 049, Loss: 0.6066\n",
      "Epoch: 050, Loss: 0.6286\n",
      "Epoch: 051, Loss: 0.5959\n",
      "Epoch: 052, Loss: 0.5649\n",
      "Epoch: 053, Loss: 0.5583\n",
      "Epoch: 054, Loss: 0.5638\n",
      "Epoch: 055, Loss: 0.5609\n",
      "Epoch: 056, Loss: 0.5229\n",
      "Epoch: 057, Loss: 0.5018\n",
      "Epoch: 058, Loss: 0.4914\n",
      "Epoch: 059, Loss: 0.4914\n",
      "Epoch: 060, Loss: 0.4807\n",
      "Epoch: 061, Loss: 0.4426\n",
      "Epoch: 062, Loss: 0.4720\n",
      "Epoch: 063, Loss: 0.4233\n",
      "Epoch: 064, Loss: 0.4559\n",
      "Epoch: 065, Loss: 0.4433\n",
      "Epoch: 066, Loss: 0.4560\n",
      "Epoch: 067, Loss: 0.4005\n",
      "Epoch: 068, Loss: 0.4231\n",
      "Epoch: 069, Loss: 0.4226\n",
      "Epoch: 070, Loss: 0.3979\n",
      "Epoch: 071, Loss: 0.3776\n",
      "Epoch: 072, Loss: 0.4103\n",
      "Epoch: 073, Loss: 0.4137\n",
      "Epoch: 074, Loss: 0.3879\n",
      "Epoch: 075, Loss: 0.3870\n",
      "Epoch: 076, Loss: 0.3858\n",
      "Epoch: 077, Loss: 0.3816\n",
      "Epoch: 078, Loss: 0.3751\n",
      "Epoch: 079, Loss: 0.3725\n",
      "Epoch: 080, Loss: 0.3674\n",
      "Epoch: 081, Loss: 0.3685\n",
      "Epoch: 082, Loss: 0.3819\n",
      "Epoch: 083, Loss: 0.3884\n",
      "Epoch: 084, Loss: 0.3662\n",
      "Epoch: 085, Loss: 0.3244\n",
      "Epoch: 086, Loss: 0.3655\n",
      "Epoch: 087, Loss: 0.3087\n",
      "Epoch: 088, Loss: 0.3411\n",
      "Epoch: 089, Loss: 0.3744\n",
      "Epoch: 090, Loss: 0.3369\n",
      "Epoch: 091, Loss: 0.3539\n",
      "Epoch: 092, Loss: 0.3581\n",
      "Epoch: 093, Loss: 0.3189\n",
      "Epoch: 094, Loss: 0.3577\n",
      "Epoch: 095, Loss: 0.3477\n",
      "Epoch: 096, Loss: 0.3341\n",
      "Epoch: 097, Loss: 0.3721\n",
      "Epoch: 098, Loss: 0.3149\n",
      "Epoch: 099, Loss: 0.2930\n",
      "Epoch: 100, Loss: 0.3434\n",
      "Epoch: 101, Loss: 0.2837\n",
      "Epoch: 102, Loss: 0.2895\n",
      "Epoch: 103, Loss: 0.2941\n",
      "Epoch: 104, Loss: 0.2894\n",
      "Epoch: 105, Loss: 0.3216\n",
      "Epoch: 106, Loss: 0.3322\n",
      "Epoch: 107, Loss: 0.3013\n",
      "Epoch: 108, Loss: 0.2867\n",
      "Epoch: 109, Loss: 0.3436\n",
      "Epoch: 110, Loss: 0.2717\n",
      "Epoch: 111, Loss: 0.2711\n",
      "Epoch: 112, Loss: 0.2915\n",
      "Epoch: 113, Loss: 0.3047\n",
      "Epoch: 114, Loss: 0.3306\n",
      "Epoch: 115, Loss: 0.3048\n",
      "Epoch: 116, Loss: 0.2979\n",
      "Epoch: 117, Loss: 0.3102\n",
      "Epoch: 118, Loss: 0.2683\n",
      "Epoch: 119, Loss: 0.2997\n",
      "Epoch: 120, Loss: 0.2498\n",
      "Epoch: 121, Loss: 0.2970\n",
      "Epoch: 122, Loss: 0.3031\n",
      "Epoch: 123, Loss: 0.2766\n",
      "Epoch: 124, Loss: 0.2801\n",
      "Epoch: 125, Loss: 0.2928\n",
      "Epoch: 126, Loss: 0.2676\n",
      "Epoch: 127, Loss: 0.2927\n",
      "Epoch: 128, Loss: 0.2907\n",
      "Epoch: 129, Loss: 0.2524\n",
      "Epoch: 130, Loss: 0.2667\n",
      "Epoch: 131, Loss: 0.2940\n",
      "Epoch: 132, Loss: 0.2887\n",
      "Epoch: 133, Loss: 0.2574\n",
      "Epoch: 134, Loss: 0.2579\n",
      "Epoch: 135, Loss: 0.2844\n",
      "Epoch: 136, Loss: 0.2541\n",
      "Epoch: 137, Loss: 0.2420\n",
      "Epoch: 138, Loss: 0.2421\n",
      "Epoch: 139, Loss: 0.2968\n",
      "Epoch: 140, Loss: 0.2336\n",
      "Epoch: 141, Loss: 0.2413\n",
      "Epoch: 142, Loss: 0.2513\n",
      "Epoch: 143, Loss: 0.2642\n",
      "Epoch: 144, Loss: 0.2511\n",
      "Epoch: 145, Loss: 0.2776\n",
      "Epoch: 146, Loss: 0.2519\n",
      "Epoch: 147, Loss: 0.2312\n",
      "Epoch: 148, Loss: 0.2575\n",
      "Epoch: 149, Loss: 0.2504\n",
      "Epoch: 150, Loss: 0.2247\n",
      "Epoch: 151, Loss: 0.2489\n",
      "Epoch: 152, Loss: 0.2229\n",
      "Epoch: 153, Loss: 0.2844\n",
      "Epoch: 154, Loss: 0.2554\n",
      "Epoch: 155, Loss: 0.2551\n",
      "Epoch: 156, Loss: 0.2402\n",
      "Epoch: 157, Loss: 0.2499\n",
      "Epoch: 158, Loss: 0.2436\n",
      "Epoch: 159, Loss: 0.2573\n",
      "Epoch: 160, Loss: 0.2204\n",
      "Epoch: 161, Loss: 0.2614\n",
      "Epoch: 162, Loss: 0.2512\n",
      "Epoch: 163, Loss: 0.2422\n",
      "Epoch: 164, Loss: 0.2564\n",
      "Epoch: 165, Loss: 0.2462\n",
      "Epoch: 166, Loss: 0.2268\n",
      "Epoch: 167, Loss: 0.2596\n",
      "Epoch: 168, Loss: 0.2509\n",
      "Epoch: 169, Loss: 0.2564\n",
      "Epoch: 170, Loss: 0.2304\n",
      "Epoch: 171, Loss: 0.2718\n",
      "Epoch: 172, Loss: 0.2336\n",
      "Epoch: 173, Loss: 0.2435\n",
      "Epoch: 174, Loss: 0.2500\n",
      "Epoch: 175, Loss: 0.2331\n",
      "Epoch: 176, Loss: 0.2453\n",
      "Epoch: 177, Loss: 0.2165\n",
      "Epoch: 178, Loss: 0.2205\n",
      "Epoch: 179, Loss: 0.2420\n",
      "Epoch: 180, Loss: 0.2424\n",
      "Epoch: 181, Loss: 0.2561\n",
      "Epoch: 182, Loss: 0.2189\n",
      "Epoch: 183, Loss: 0.2317\n",
      "Epoch: 184, Loss: 0.2154\n",
      "Epoch: 185, Loss: 0.2353\n",
      "Epoch: 186, Loss: 0.2115\n",
      "Epoch: 187, Loss: 0.2181\n",
      "Epoch: 188, Loss: 0.2280\n",
      "Epoch: 189, Loss: 0.2290\n",
      "Epoch: 190, Loss: 0.2224\n",
      "Epoch: 191, Loss: 0.2323\n",
      "Epoch: 192, Loss: 0.2126\n",
      "Epoch: 193, Loss: 0.2198\n",
      "Epoch: 194, Loss: 0.2125\n",
      "Epoch: 195, Loss: 0.2084\n",
      "Epoch: 196, Loss: 0.2136\n",
      "Epoch: 197, Loss: 0.2264\n",
      "Epoch: 198, Loss: 0.2062\n",
      "Epoch: 199, Loss: 0.2048\n",
      "Epoch: 200, Loss: 0.2072\n"
     ]
    }
   ],
   "source": [
    "num_hidden_features = 64\n",
    "model = GCN(dataset.num_node_features, num_hidden_features, dataset.num_classes)\n",
    "criterion = torch.nn.CrossEntropyLoss()  # Define loss criterion.\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)  # Define optimizer.\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()  # Clear gradients.\n",
    "      out = model(data.x, edge_index)  # Perform a single forward pass.\n",
    "      loss = criterion(out[data.train_mask], data.y[data.train_mask])  # Compute the loss solely based on the training nodes.\n",
    "      loss.backward()  # Derive gradients.\n",
    "      optimizer.step()  # Update parameters based on gradients.\n",
    "      return loss\n",
    "\n",
    "def test():\n",
    "      model.eval()\n",
    "      out = model(data.x, edge_index)\n",
    "      pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "      test_correct = pred[data.test_mask] == data.y[data.test_mask]  # Check against ground-truth labels.\n",
    "      test_acc = int(test_correct.sum()) / int(data.test_mask.sum())  # Derive ratio of correct predictions.\n",
    "      return test_acc\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "74dcc702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8170\n"
     ]
    }
   ],
   "source": [
    "test_acc = test()\n",
    "print(f'Test Accuracy: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3bf0e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd51b6dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hyperbolic_optimization_srp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
